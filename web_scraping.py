# -*- coding: utf-8 -*-
"""Web Scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRCmPbVnfbrq8zVbCZY_FhFr11Ar6tSG
"""

# Web scraping in Python refers to the automated process of extracting data from websites.
# It involves using libraries like Beautiful Soup and Scrapy to fetch the HTML content of a webpage,
# parse it, and extract specific information.  This extracted data can then be saved to a file, database, or used for further analysis.
# Ethical considerations and website terms of service must always be respected when scraping.

from bs4 import BeautifulSoup as bs
import requests as r

url = 'https://www.scrapethissite.com/pages/'
page = requests.get(url)
soup = BeautifulSoup(page.text, 'html.parser')
print(soup)

link = 'https://en.wikipedia.org/wiki/Web_scraping'
p = requests.get(link)
soup1 = BeautifulSoup(p.text , 'html.parser')
print(soup1)

link1 = 'https://www.imperva.com/learn/application-security/data-scraping/'
a = requests.get(link1)
soup2 = BeautifulSoup(a.text , 'html.parser')
print(soup2)

link2 = 'https://github.com/'
g = requests.get(link2)
soup3 = BeautifulSoup(g.text , 'html.parser')
print(soup3)

link3 = 'https://www.amazon.com/s?k=gaming+keyboard&_encoding=UTF8&content-id=amzn1.sym.12129333-2117-4490-9c17-6d31baf0582a&pd_rd_r=17ff603e-81c8-4c88-831a-a60e29953f46&pd_rd_w=CzhCp&pd_rd_wg=mDBiK&pf_rd_p=12129333-2117-4490-9c17-6d31baf0582a&pf_rd_r=R3G8PRAPC1HQPKS6FN1G&ref=pd_hp_d_atf_unk'
e = requests.get(link3)
soup4 = BeautifulSoup(e.text , 'html.parser')
print(soup4)

soup2.find('h1').text

soup.find_all('h1')[0]

p=(bs(r.get('https://timesofindia.indiatimes.com/defaultinterstitial.cms?b=/').text , 'html.parser'))

p.find("h1")

s=bs(r.get('https://epaper.indiatimes.com/timesepaper/publication-the-times-of-india,city-delhi.cms').text , 'html.parser')

s.find("h1").text

t=bs(r.get('https://timesofindia.indiatimes.com/').text , 'html.parser')

t.find("h1").text

w=bs(r.get('https://timesofindia.indiatimes.com/defaultinterstitial.cms?b=/').text , 'html.parser')

w.find("h1")

soup2.find('h2').text

soup2.find_all("body")

soup2.find_all("div")

soup2.find("p")

lnk1 = 'https://www.uhdpaper.com/'
ge1 = requests.get(lnk1)
if ge1.status_code == 200:
  print('Success')
else:
  print('Tere bas ki baat nahi')

lnk2 ='https://web.whatsapp.com/'
ge2 = requests.get(lnk2)
so2 = BeautifulSoup(ge2.text , 'html')

so2.find('body').text

#  Extract Headlines from BBC News Goal: Extract top 5 headlines from https://www.bbc.com/news
def get_bbc_headlines():
    url = "https://www.bbc.com/news"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    headlines = soup.find_all('h3')
    for i, h in enumerate(headlines[:5]):
        print(f"{i+1}. {h.get_text(strip=True)}")

get_bbc_headlines()

# Get All Hyperlinks from a Website Goal: Extract and print all tags (text + href) from https://example.com
def extract_links():
    url = "https://example.com"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html')

    links = soup.find_all('a')
    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href')
        if href:
            print(f"Text: {text} | Link: {href}")

extract_links()

# Scrape Book Titles from books.toscrape.com Goal: Get titles of first 10 books.
def scrape_book_titles():
    url = "http://books.toscrape.com"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html')

    books = soup.find_all('h3')[:10]
    for i, book in enumerate(books, 1):
        title = book.a['title']
        print(f"{i}. {title}")

scrape_book_titles()

# Extract Quotes and Authors Goal: Get 5 quotes and their authors from http://quotes.toscrape.com
def scrape_quotes():
    url = "http://quotes.toscrape.com"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    quotes = soup.find_all('div', class_='quote')[:5]
    for i, quote in enumerate(quotes, 1):
        text = quote.find('span', class_='text').get_text(strip=True)
        author = quote.find('small', class_='author').get_text(strip=True)
        print(f"{i}. {text} â€” {author}")

scrape_quotes()

